<!doctype html><html lang="en" class="no-js"><head><meta charset="utf-8"> <!-- begin SEO --><title>Traffic Sign Recognition with Tensorflow - Giovanni Claudio - Personal Website</title><meta property="og:locale" content="en-US"><meta property="og:site_name" content="Giovanni Claudio - Personal Website"><meta property="og:title" content="Traffic Sign Recognition with Tensorflow"><link rel="canonical" href="index.html"><meta property="og:url" content="http://jokla.me/robotics/traffic-signs/"><meta property="og:description" content="Designed a CNN inspired by LeNet architecture"><meta property="og:type" content="article"><meta property="article:published_time" content="2017-03-31T00:00:00+00:00"> <script type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Giovanni Claudio", "url" : "http://jokla.me", "sameAs" : null } </script> <!-- end SEO --><link href="../../feed.xml" type="application/atom+xml" rel="alternate" title="Giovanni Claudio - Personal Website Feed"> <!-- http://t.co/dKP3o1e --><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width, initial-scale=1.0"> <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script> <!-- For all browsers --><link rel="stylesheet" href="../../assets/css/main.css"><meta http-equiv="cleartype" content="on"> <!-- start custom head snippets --> <!-- insert favicons. use http://realfavicongenerator.net/ --> <!-- end custom head snippets --></head><body> <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]--><div class="masthead"><div class="masthead__inner-wrap"><div class="masthead__menu"><nav id="site-nav" class="greedy-nav"> <button><div class="navicon"></div></button><ul class="visible-links"><li class="masthead__menu-item masthead__menu-item--lg"><a href="../../index.html">Giovanni Claudio - Personal Website</a></li><li class="masthead__menu-item"><a href="../../about/index.html">About me</a></li><li class="masthead__menu-item"><a href="../../posts/index.html">Posts</a></li><li class="masthead__menu-item"><a href="../../work/index.html">Work</a></li><li class="masthead__menu-item"><a href="../../software/index.html">Software</a></li><li class="masthead__menu-item"><a href="../../music/index.html">Music</a></li></ul><ul class="hidden-links hidden"></ul></nav></div></div></div><div id="main" role="main"><div class="sidebar sticky"><div itemscope itemtype="http://schema.org/Person"><div class="author__avatar"> <img src="../../images/bio-photo.jpg" class="author__avatar" alt="Giovanni Claudio"></div><div class="author__content"><h3 class="author__name">Giovanni Claudio</h3><p class="author__bio">Autonomous Driving Engineer</p></div><div class="author__urls-wrapper"> <button class="btn btn--inverse">Follow</button><ul class="author__urls social-icons"><li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Turin</li><li><a href="https://www.linkedin.com/in/giovanniclaudio"><i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn</a></li><li><a href="https://github.com/jokla"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li><li><a href="https://www.youtube.com/user/Jokla89zena"><i class="fa fa-fw fa-youtube-square" aria-hidden="true"></i> YouTube</a></li></ul></div></div></div><article class="page" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="headline" content="Traffic Sign Recognition with Tensorflow"><meta itemprop="description" content="Designed a CNN inspired by LeNet architecture"><meta itemprop="datePublished" content="March 31, 2017"><meta itemprop="dateModified" content="March 31, 2017"><div class="page__inner-wrap"><header><h1 class="page__title" itemprop="headline">Traffic Sign Recognition with Tensorflow</h1></header><section class="page__content" itemprop="text"><h1 id="introduction">Introduction</h1><p>In this project, I used a convolutional neural network (CNN) to classify traffic signs. I trained and validated a model so it can classify traffic sign images using the <a href="http://benchmark.ini.rub.de/">German Traffic Sign Dataset</a>. After the model is trained, I tried out the model on images of traffic signs that I took with my smartphone camera.</p><p>My final model results are:</p><ul><li>Training set accuracy of 97.5%</li><li>Validation set accuracy of 98.5%</li><li>Test set accuracy of 97.2%</li><li>New Test set accuracy of 100% (6 new images taken by me)</li></ul><p>Here is the <a href="https://github.com/jokla/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb">project code</a>. Please note that I used only the CPU of my laptop to train the network.</p><p>The steps of this project are the following:</p><ul><li>Load the data set</li><li>Explore, summarize and visualize the data set</li><li>Design, train and test a model architecture</li><li>Use the model to make predictions on new images</li><li>Analyze the softmax probabilities of the new images</li><li>Summarize the results with a written report</li></ul><h1 id="data-set-summary--exploration">Data Set Summary &amp; Exploration</h1><h2 id="1-basic-summary-of-the-data-set">1. Basic summary of the data set</h2><p>I used the Pandas library to calculate summary statistics of the traffic signs data set:</p><ul><li>The size of original training set is 34799</li><li>The size of the validation set is 4410</li><li>The size of test set is 12630</li><li>The shape of a traffic sign image is 32x32x3 represented as integer values (0-255) in the RGB color space</li><li>The number of unique classes/labels in the data set is 43</li></ul><p>We have to work with images with a resolution of 32x32x3 representing 43 type of different German traffic signs.</p><h2 id="2-exploratory-visualization-of-the-dataset">2. Exploratory visualization of the dataset</h2><p>Here is an exploratory visualization of the data set. It is a bar chart showing how many samples we have for each class.</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/dist_class_training.png" width="500" alt="Distribution Class Training" /></p><p>We can notice that the distribution is not balanced. We have some classes with less than 300 examples and other well represented with more than 1000 examples. We can analyze now the validation dataset distribution:</p><p><img src="http://jokla.me/robotics/traffic-signs/.https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/dist_class_validation.png" width="500" alt="Distribution Class Validation" /></p><p>The distributions are very similar. Even if it would be wise to balance the dataset, in this case, I am not sure it would be very useful. In fact, some traffic signs (for example the 20km/h speed limit) could occur less frequently than others (the stop sign for example). For this reason I decided to not balance the dataset.</p><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Class 0: Speed limit (20km/h)                                180 samples
Class 1: Speed limit (30km/h)                                1980 samples
Class 2: Speed limit (50km/h)                                2010 samples
Class 3: Speed limit (60km/h)                                1260 samples
Class 4: Speed limit (70km/h)                                1770 samples
Class 5: Speed limit (80km/h)                                1650 samples
Class 6: End of speed limit (80km/h)                         360 samples
Class 7: Speed limit (100km/h)                               1290 samples
Class 8: Speed limit (120km/h)                               1260 samples
Class 9: No passing                                          1320 samples
Class 10: No passing for vehicles over 3.5 metric tons        1800 samples
Class 11: Right-of-way at the next intersection               1170 samples
Class 12: Priority road                                       1890 samples
Class 13: Yield                                               1920 samples
Class 14: Stop                                                690 samples
Class 15: No vehicles                                         540 samples
Class 16: Vehicles over 3.5 metric tons prohibited            360 samples
Class 17: No entry                                            990 samples
Class 18: General caution                                     1080 samples
Class 19: Dangerous curve to the left                         180 samples
Class 20: Dangerous curve to the right                        300 samples
Class 21: Double curve                                        270 samples
Class 22: Bumpy road                                          330 samples
Class 23: Slippery road                                       450 samples
Class 24: Road narrows on the right                           240 samples
Class 25: Road work                                           1350 samples
Class 26: Traffic signals                                     540 samples
Class 27: Pedestrians                                         210 samples
Class 28: Children crossing                                   480 samples
Class 29: Bicycles crossing                                   240 samples
Class 30: Beware of ice/snow                                  390 samples
Class 31: Wild animals crossing                               690 samples
Class 32: End of all speed and passing limits                 210 samples
Class 33: Turn right ahead                                    599 samples
Class 34: Turn left ahead                                     360 samples
Class 35: Ahead only                                          1080 samples
Class 36: Go straight or right                                330 samples
Class 37: Go straight or left                                 180 samples
Class 38: Keep right                                          1860 samples
Class 39: Keep left                                           270 samples
Class 40: Roundabout mandatory                                300 samples
Class 41: End of no passing                                   210 samples
Class 42: End of no passing by vehicles over 3.5 metric tons  210 samples

</code></pre></div></div><h1 id="design-and-test-a-model-architecture">Design and Test a Model Architecture</h1><h2 id="1-pre-processing">1. Pre-processing</h2><p>This phase is crucial to improving the performance of the model. First of all, I decided to convert the RGB image into grayscale color. This allows to reduce the numbers of channels in the input of the network without decreasing the performance. In fact, as Pierre Sermanet and Yann LeCun mentioned in their paper <a href="http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf">“Traffic Sign Recognition with Multi-Scale Convolutional Networks”</a>, using color channels did not seem to improve the classification accuracy. Also, to help the training phase, I normalized each image to have a range from 0 to 1 and translated to get zero mean. I also applied the <a href="https://en.wikipedia.org/wiki/Adaptive_histogram_equalization">Contrast Limited Adaptive Histogram Equalization</a> (CLAHE), an algorithm for local contrast enhancement, which uses histograms computed over different tile regions of the image. Local details can, therefore, be enhanced even in areas that are darker or lighter than most of the image. This should help the feature exaction.</p><p>Here the function I used to pre-process each image in the dataset:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pre_processing_single_img</span> <span class="p">(</span><span class="n">img</span><span class="p">):</span>

    <span class="n">img_y</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2YUV</span><span class="p">))[:,:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">img_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_y</span> <span class="o">/</span> <span class="mf">255.</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">img_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">exposure</span><span class="o">.</span><span class="n">equalize_adapthist</span><span class="p">(</span><span class="n">img_y</span><span class="p">,)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">img_y</span> <span class="o">=</span> <span class="n">img_y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img_y</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

    <span class="k">return</span> <span class="n">img_y</span>

</code></pre></div></div><p>Steps:</p><ul><li><p>Convert the image to <a href="https://en.wikipedia.org/wiki/YUV">YUV</a> and extract Y Channel that correspond to the grayscale image:<br /> <code class="highlighter-rouge">img_y = cv2.cvtColor(img, (cv2.COLOR_BGR2YUV))[:,:,0]</code> Y stands for the luma component (the brightness) and U and V are the chrominance (color) components.</p></li><li><p>Normalize the image to have a range from 0 to 1: <br /> ` img_y = (img_y / 255.).astype(np.float32) `</p></li><li><p>Contrast Limited Adaptive Histogram Equalization (see <a href="http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_adapthist">here</a> for more information) and translate the result to have mean zero: <br /> <code class="highlighter-rouge">img_y = (exposure.equalize_adapthist(img_y,) - 0.5)</code></p></li><li><p>Finally reshape the image from (32x32) to (32x32x1), the format required by tensorflow: <br /> <code class="highlighter-rouge">img_y = img_y.reshape(img_y.shape + (1,))</code></p></li></ul><p>Here is an example of a traffic sign image before and after the processing:</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/original_samples3.png" width="360" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/prepro_train.png" width="360" /></p><p>Initially, I used <code class="highlighter-rouge">.exposure.adjust_log</code>, that it is quite fast but finally I decided to use <code class="highlighter-rouge">exposure.equalize_adapthist</code>, that gives a better accuracy.</p><h2 id="2-augmentation">2. Augmentation</h2><p>To add more data to the data set, I created two new datasets starting from the original training dataset, composed by 34799 examples. In this way, I obtain 34799x3 = 104397 samples in the training dataset.</p><h2 id="keras-imagedatagenerator">Keras ImageDataGenerator</h2><p>I used the Keras function <a href="https://keras.io/preprocessing/image/">ImageDataGenerator</a> to generate new images with the following settings:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
        <span class="n">rotation_range</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span>
        <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
        <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">dim_ordering</span><span class="o">=</span><span class="s">'tf'</span><span class="p">,</span>
        <span class="n">fill_mode</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>

 <span class="c"># configure batch size and retrieve one batch of images</span>

<span class="k">for</span> <span class="n">X_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">datagen</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">X_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">X_train_aug</span> <span class="o">=</span> <span class="n">X_batch</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'uint8'</span><span class="p">)</span>
    <span class="n">y_train_aug</span> <span class="o">=</span> <span class="n">y_batch</span>
    <span class="k">break</span>
</code></pre></div></div><p>To each picture in the training dataset, a rotation, a translation, a zoom and a shear transformation is applied.</p><p>Here is an example of an original image and an augmented image:</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/original_samples.png" width="360" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/keras_prepro_samples.png" width="360" /></p><h2 id="motion-blur">Motion Blur</h2><p>Motion blur is the apparent streaking of rapidly moving objects in a still image. I thought it is a good idea add motion blur to the image since they are taken from a camera placed on a moving car.</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/original_samples1.png" width="360" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/mb_prepro_samples.png" width="360" /></p><h2 id="3-final-model-architecture">3. Final model architecture</h2><p>I started from the LeNet network and I modified it using the multi-scale features took inspiration from the model presented in <a href="http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf">Pierre Sermanet and Yann LeCun</a> paper. Finally, I increased the number of filters used in the first two convolutions. We have in total 3 layers: 2 convolutional layers for feature extraction and one fully connected layer used. Note that my network has one convolutional layer less than the [Pierre Sermanet and Yann LeCun (http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf) version.</p><table><thead><tr><th style="text-align: center">Layer</th><th style="text-align: center">Description</th></tr></thead><tbody><tr><td style="text-align: center">Input</td><td style="text-align: center">32x32x1 Grayscale image</td></tr><tr><td style="text-align: center">Convolution 3x3</td><td style="text-align: center">1x1 stride, same padding, outputs 28x28x12</td></tr><tr><td style="text-align: center">RELU</td><td style="text-align: center"> </td></tr><tr><td style="text-align: center">Max pooling</td><td style="text-align: center">2x2 stride, outputs 14x14x12</td></tr><tr><td style="text-align: center">Dropout (a)</td><td style="text-align: center">0.7</td></tr><tr><td style="text-align: center">Convolution 3x3</td><td style="text-align: center">1x1 stride, same padding, outputs 10x10x24</td></tr><tr><td style="text-align: center">RELU</td><td style="text-align: center"> </td></tr><tr><td style="text-align: center">Max pooling</td><td style="text-align: center">2x2 stride, output = 5x5x24.</td></tr><tr><td style="text-align: center">Dropout (b)</td><td style="text-align: center">0.6</td></tr><tr><td style="text-align: center">Fully connected</td><td style="text-align: center">max_pool(a) + (b) flattend. input = 1188. Output = 320</td></tr><tr><td style="text-align: center">Dropout (c)</td><td style="text-align: center">0.5</td></tr><tr><td style="text-align: center">Fully connected</td><td style="text-align: center">Input = 320. Output = n_classes</td></tr><tr><td style="text-align: center">Softmax</td><td style="text-align: center"> </td></tr></tbody></table><p>To train the model I used 20 epochs with a batch size of 128, the <a href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer">AdamOptimizer</a>(see paper <a href="https://arxiv.org/pdf/1412.6980v8.pdf">here</a>) with a learning rate of 0.001. The training phase is quite slow using only CPU, that’s why I used only 20 epochs.</p><p>My final model results were:</p><ul><li>Training set accuracy of 97.5%</li><li>Validation set accuracy of 98.5%</li><li>Test set accuracy of 97.2%</li></ul><h3 id="first-attempt-validation-accuracy-915">First attempt: validation accuracy 91.5%</h3><p>Initially, I started with the <a href="http://yann.lecun.com/exdb/lenet/">LeNet architecture</a>, a convolutional network designed for handwritten and machine-printed character recognition.</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/lenet5.png" width="900" /></p><p>I used the the following preprocess pipeline:</p><ul><li>Convert in YUV, keep the Y</li><li>Adjust the exposure</li><li>Normalization</li></ul><p>Parameters:</p><ul><li>EPOCHS = 10</li><li>BATCH_SIZE = 128</li><li>Learning rate = 0.001</li></ul><p>Number of training examples = 34799 <br /> Number of validation examples = 4410 <br /> Number of testing examples = 12630</p><p>At each step, I will mention only the changes I adopted to improve the accuracy.</p><h3 id="second-attempt-validation-accuracy-931">Second attempt: validation accuracy 93.1%</h3><p>I added Dropout after each layer of the network LeNet: <br /> 1) <code class="highlighter-rouge">0.9</code> (after C1) <br /> 2) <code class="highlighter-rouge">0.7</code> (after C3) <br /> 3) <code class="highlighter-rouge">0.6</code> (after C5) <br /> 4) <code class="highlighter-rouge">0.5</code> (after F6)</p><h3 id="third-attempt-validation-accuracy-933">Third attempt: validation accuracy 93.3%</h3><p>I changed the network using multi-scale features as suggested in the paper <a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi079aWzOjSAhWHJ8AKHUx_ARkQFggdMAA&amp;url=http%3A%2F%2Fyann.lecun.org%2Fexdb%2Fpublis%2Fpsgz%2Fsermanet-ijcnn-11.ps.gz&amp;usg=AFQjCNGTHlNOHKmIxaKYw3_h-VYrsgpCag&amp;sig2=llvR7_9QizK3hkAgkmUKTw">Traffic Sign Recognition with Multi-Scale Convolutional Networks</a> and use only one fully connected layer at the end of the network.</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/net.png" width="800" /></p><h3 id="fourth-attempt-validation-accuracy-946">Fourth attempt: validation accuracy 94.6%</h3><p>I augmented the training set using the Keras function <a href="https://keras.io/preprocessing/image/">ImageDataGenerator</a>. In this way, I double the training set.</p><p>Number of training examples = 34799x2 = 69598 <br /> Number of validation examples = 4410</p><p>I Used Dropout with the follow probability (referred <a href="https://github.com/jokla/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup.md#3-final-model-architecture">this table</a>): <br /> a) <code class="highlighter-rouge">0.8</code> <br /> b) <code class="highlighter-rouge">0.7</code> <br /> c) <code class="highlighter-rouge">0.6 </code></p><h3 id="fifth-attempt-validation-accuracy-961">Fifth attempt: validation accuracy 96.1%</h3><p>Since the training accuracy was not very high, I decided to increase the number of filters in the first two convolutional layers. <br /> First layer: from 6 to 12 filters. <br /> Second layer: from 16 to 24 filters.</p><h3 id="final-attempt-validation-accuracy-985">Final attempt: validation accuracy 98.5%</h3><p>I augmented the data adding the motion blur to each sample of the training data. Hence, I triplicate the number of samples in the training set. In addition, I added the L2 regularization and I used the function <code class="highlighter-rouge">equalize_adapthist</code> instead of <code class="highlighter-rouge">.exposure.adjust_log</code> during the image preprocessing.</p><h2 id="performance-on-the-test-set">Performance on the test set</h2><p>Finally I evaluated the performance of my model with the test set.</p><h3 id="accuracy">Accuracy</h3><p>The accuracy was equal to 97.2%.</p><h3 id="precision">Precision</h3><p>The Precision was equal to 96.6% <br /> The precision is the ratio <code class="highlighter-rouge">tp / (tp + fp)</code> where <code class="highlighter-rouge">tp</code> is the number of true positives and <code class="highlighter-rouge">fp</code> the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.</p><h3 id="recall">Recall</h3><p>The recall was equal to 97.2% The recall is the ratio <code class="highlighter-rouge">tp / (tp + fn)</code> where <code class="highlighter-rouge">tp</code> is the number of true positives and <code class="highlighter-rouge">fn</code> the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.</p><h3 id="confusion-matrix">Confusion matrix</h3><p>Let’s analyze the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>:</p><div> <a href="https://plot.ly/~jokla/1/?share_key=DmJjGBAv9EQXNjMc5jDWCT" target="_blank" title="Plot 1" style="display: block; text-align: center;"><img src="https://plot.ly/~jokla/1.png?share_key=DmJjGBAv9EQXNjMc5jDWCT" alt="Plot 1" style="max-width: 100%;width: 600px;" width="900" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a></div><p>You can click on the picture to interact with the map.</p><p>We can notice that:</p><ul><li>28/60 samples of to the class 19 (Dangerous curve to the left) are misclassified as samples belonging to the class 23 (Slippery road). This can be explained by the fact that the class number 19 is underrepresented in the training set: it has only 180 samples.</li><li>34/630 samples of to the class 5 (Speed limit 80km/h) are misclassified as samples belonging to the class 2 (Speed limit 50km/h).</li><li>The model is not very good to classify the class number 30 (Beware of ice/snow), it classified samples in a wrong way 61 times.</li><li>The model produces 80 false positives for the class 23.</li></ul><h1 id="test-a-model-on-new-images">Test a Model on New Images</h1><p>Here are five traffic signs from some pictures I took in France with my smartphone: <br /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/11_Rightofway.jpg" width="100" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/25_RoadWork.jpg" width="100" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/14_Stop.jpg" width="100" /> <br /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/17_Noentry.jpg" width="100" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/12_PriorityRoad.jpg" width="100" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/33_RightOnly.jpg" width="100" /></p><p>Here are the results of the prediction:</p><ul><li>the first image is the test image</li><li>the second one is a random picture of the same class of the prediction</li><li>the third one is a plot showing the top five soft max probabilities</li></ul><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/new_sign1.png" width="480" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/new_sign2.png" width="480" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/new_sign3.png" width="480" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/new_sign4.png" width="480" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/new_sign5.png" width="480" /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/new_sign6.png" width="480" /></p><p>The model was able to correctly guess 6 of the 6 traffic signs, which gives an accuracy of 100%. Nice!</p><h1 id="visualize-the-neural-networks-state-with-test-images">Visualize the Neural Network’s State with Test Images</h1><p>We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it’s feature maps look like by plotting the output of the network’s weight layers in response to a test stimuli image. From these plotted feature maps, it’s possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign’s boundary outline or the contrast in the sign’s painted symbol.</p><p>Here the output of the first convolution layer:<br /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/visualize1.png" width="700" /> <br /> Here the output of the second convolution layer:<br /> <img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/visualize2.png" width="700" /></p><p>We can notice that the CNN learned to detect useful features on its own. We can see in the first picture some edges of the sign for example.</p><p>Now another example using a test picture with no sign on it:</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/no_sign.png" width="100" /></p><p>In this case the CNN does not recognize any useful features. The activations of the first feature map appear to contain mostly noise:</p><p><img src="https://raw.githubusercontent.com/jokla/jokla.github.io/master/images/post/2017-03-31-traffic-signs/visualize1_nosign.png" width="700" /></p><h1 id="final-considerations">Final considerations</h1><h2 id="premise">Premise:</h2><p>Only CPU was used to train the network. I had to use only the CPU of my laptop because I didn’t have any good GPU at my disposal. I choose to not use any online service like AWS or FloydHub, mostly because I was waiting for the arrival of the GTX 1080. Unfortunately, it did not arrive in time for this project. This required me to use a small network and to keep the number of epochs around 20.</p><h2 id="some-possible-improvements">Some possible improvements:</h2><ul><li>I would use Keras to define the network and its function ImageDataGenerator to generate augmented samples on the fly. Using more data could improve the performance of the model. In my case, I have generated an augmented dataset once, saved it on the disk and used it every time to train. It would be useful to generate randomly the dataset each time before the training.</li><li>The confusion matrix gives us suggestions to improve the model (see section <code class="highlighter-rouge">Confusion matrix</code>). There are some classes with low precision or recall. It would be useful to try to add more data for these classes. For example, I would generate new samples for the class 19 (Dangerous curve to the left) since it has only 180 samples and the model.</li><li>The accuracy for the training set is 0.975. This means that the model is probably underfitting a little bit. I tried to make a deeper network (adding more layers) and increasing the number of filters but it was too slow to train it using the CPU only.</li><li>The model worked well with new images taken with my camera (100% of accuracy). It would be useful to test the model by using more complicated examples.</li></ul></section><footer class="page__meta"><p class="page__taxonomy"> <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong> <span itemprop="keywords"> <a href="../../tags/index.html#deep-learning" class="page__taxonomy-item" rel="tag">Deep Learning</a><span class="sep">, </span> <a href="../../tags/index.html#tensorflow" class="page__taxonomy-item" rel="tag">Tensorflow</a> </span></p><p class="page__taxonomy"> <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong> <span itemprop="keywords"> <a href="../../categories/index.html#robotics" class="page__taxonomy-item" rel="tag">Robotics</a> </span></p><p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-03-31">March 31, 2017</time></p></footer><section class="page__share"><h4 class="page__share-title">Share on</h4><a href="https://twitter.com/intent/tweet?text=http://jokla.me/robotics/traffic-signs/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a> <a href="https://www.facebook.com/sharer/sharer.php?u=http://jokla.me/robotics/traffic-signs/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a> <a href="https://plus.google.com/share?url=http://jokla.me/robotics/traffic-signs/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a> <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://jokla.me/robotics/traffic-signs/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a></section><nav class="pagination"> <a href="../lane-detection/index.html" class="pagination--pager" title="Finding Lane Lines on the Road ">Previous</a> <a href="index.html#" class="pagination--pager disabled">Next</a></nav></div><div class="page__comments"><h4 class="page__comments-title">Leave a Comment</h4><section id="disqus_thread"></section></div></article><div class="page__related"><h4 class="page__related-title">You May Also Enjoy</h4><div class="grid__wrapper"><div class="grid__item"><article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="archive__item-title" itemprop="headline"> <a href="../lane-detection/index.html" rel="permalink">Finding Lane Lines on the Road </a></h2><p class="archive__item-excerpt" itemprop="description">First project of the Udacity Self Driving Car NanoDegree</p></article></div><div class="grid__item"><article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="archive__item-title" itemprop="headline"> <a href="../../self-driving-car/install-unity3d-ubuntu/index.html" rel="permalink">Install Unity3D in Ubuntu </a></h2><p class="archive__item-excerpt" itemprop="description">Tested in Ubuntu 14.04</p></article></div><div class="grid__item"><article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="archive__item-title" itemprop="headline"> <a href="../lookup-transform-ros/index.html" rel="permalink">How to compute frame transformations with Tf (ROS) </a></h2><p class="archive__item-excerpt" itemprop="description">using the function lookupTransform</p></article></div><div class="grid__item"><article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="archive__item-title" itemprop="headline"> <a href="../camera-calibration-visp/index.html" rel="permalink">Intrinsic camera calibration for Nao/Romeo/Pepper with Visp </a></h2><p class="archive__item-excerpt" itemprop="description">Intrinsic camera calibration for Nao/Romeo/Pepper with Visp.</p></article></div></div></div></div><div class="page__footer"><footer> <!-- start custom footer snippets --> <!-- end custom footer snippets --><div class="page__footer-follow"><ul class="social-icons"><li><strong>Follow:</strong></li><li><a href="http://github.com/jokla"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li><li><a href="../../feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li></ul></div><div class="page__footer-copyright">&copy; 2017 Giovanni Claudio. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div></footer></div><script src="../../assets/js/main.min.js"></script> <script type="text/javascript"> var _gaq = _gaq || []; _gaq.push(['_setAccount', 'UA-57751247-1']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })(); </script> <script type="text/javascript"> /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */ var disqus_shortname = 'joklasite'; /* * * DON'T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js'; (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })(); /* * * DON'T EDIT BELOW THIS LINE * * */ (function () { var s = document.createElement('script'); s.async = true; s.type = 'text/javascript'; s.src = '//' + disqus_shortname + '.disqus.com/count.js'; (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s); }()); </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></body></html>
